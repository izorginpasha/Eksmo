import whisper
import requests
from pathlib import Path
import pandas as pd
import re
from utilities.utilities import save_events_to_excel

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
model = whisper.load_model("medium")

def get_sfx_from_ollama(text_segment):
    prompt = f"""
–¢—ã ‚Äî –∑–≤—É–∫–æ–≤–æ–π –¥–∏–∑–∞–π–Ω–µ—Ä. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç –∏ –≤–µ—Ä–Ω–∏ —Å–ø–∏—Å–æ–∫ –∑–≤—É–∫–æ–≤—ã—Ö —ç—Ñ—Ñ–µ–∫—Ç–æ–≤ —Å –∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.
–§–æ—Ä–º–∞—Ç —Å—Ç—Ä–æ–≥–æ —Ç–∞–∫–æ–π:

[
  {{
    "sound": "–Ω–∞–∑–≤–∞–Ω–∏–µ.wav",
    "start": 1.5,
    "duration": 2.0,
    "volume": -5
  }}
]

–¢–æ–ª—å–∫–æ JSON, –Ω–∏–∫–∞–∫–∏—Ö –ø–æ—è—Å–Ω–µ–Ω–∏–π. –¢–µ–∫—Å—Ç: \"\"\"{text_segment}\"\"\"
"""
    try:
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={"model": "llama3", "prompt": prompt, "stream": False}
        )
        result = response.json().get("response", "").strip()
        print("üì§ –û—Ç–≤–µ—Ç –æ—Ç Ollama:", repr(result))

        match = re.search(r'\[(?:.|\n)*?\]', result)
        if match:
            return eval(match.group(0))
        return []
    except Exception as e:
        print("‚ùå –û—à–∏–±–∫–∞ Ollama:", e)
        return []

def merge_segments(segments, max_duration=10.0):
    """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å–µ–≥–º–µ–Ω—Ç—ã whisper –≤ –±–ª–æ–∫–∏ –¥–æ max_duration —Å–µ–∫—É–Ω–¥"""
    merged = []
    buffer = []
    buffer_start = None

    for seg in segments:
        if not buffer:
            buffer_start = seg['start']
        buffer.append(seg)

        total_duration = seg['end'] - buffer_start
        if total_duration >= max_duration:
            merged.append({
                "start": buffer_start,
                "end": seg['end'],
                "text": " ".join(s['text'] for s in buffer)
            })
            buffer = []

    if buffer:
        merged.append({
            "start": buffer_start,
            "end": buffer[-1]['end'],
            "text": " ".join(s['text'] for s in buffer)
        })

    return merged

def main():
    audio_path = "audio/voice.mp3"
    result = model.transcribe(audio_path, language="ru", verbose=False)
    segments = result["segments"]

    # üîÅ –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã –ø–æ 10 —Å–µ–∫—É–Ω–¥
    merged_segments = merge_segments(segments, max_duration=10.0)

    events = []
    for seg in merged_segments:
        print(f"üó£Ô∏è –°–µ–≥–º–µ–Ω—Ç (–æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π): {seg['text']}")
        sfx_list = get_sfx_from_ollama(seg['text'])
        print(f"üéµ –ù–∞–π–¥–µ–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–æ–≤: {sfx_list}")
        for sfx in sfx_list:
            events.append({
                "text": seg['text'],
                "sound": sfx["sound"],
                "start": seg["start"] + float(sfx["start"]),  # —Å–º–µ—â–µ–Ω–∏–µ –≤–Ω—É—Ç—Ä–∏ –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω–æ–≥–æ –±–ª–æ–∫–∞
                "duration": float(sfx["duration"]),
                "volume": float(sfx["volume"])
            })

    save_events_to_excel(events)

if __name__ == "__main__":
    main()