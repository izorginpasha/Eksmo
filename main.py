
import whisper
import spacy
from gtts import gTTS
from pydub import AudioSegment
from pydub.playback import play
from pydub.effects import normalize
import os
from pathlib import Path
import requests
from utilities.utilities import save_events_to_excel, mitation_acting

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏
model = whisper.load_model("base")
#-------------–ò–º–∏—Ç–∞—Ü–∏—è –æ–∑–≤—É—á–∫–∏ –¥–∏–∫—Ç–æ—Ä–∞ - -------
#mitation_acting()


# ---------------------------------------------
# üîÅ –§—É–Ω–∫—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞ –≤ LLM
def get_sfx_from_ollama(text_segment):
    prompt = f"""
    –¢—ã ‚Äî –∑–≤—É–∫–æ–≤–æ–π –¥–∏–∑–∞–π–Ω–µ—Ä.

    –ù–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞ –ø–æ–¥–±–µ—Ä–∏ —Ç–æ–ª—å–∫–æ —Å–ø–∏—Å–æ–∫ –∑–≤—É–∫–æ–≤—ã—Ö —ç—Ñ—Ñ–µ–∫—Ç–æ–≤. –í–µ—Ä–Ω–∏ —Å—Ç—Ä–æ–≥–æ —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Python, –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π.

    –ü—Ä–∏–º–µ—Ä –æ—Ç–≤–µ—Ç–∞:
    ["sound1.wav", "sound2.wav"]

    –¢–µ–∫—Å—Ç: "{text_segment}"
    """
    try:
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={"model": "llama3", "prompt": prompt, "stream": False}
        )
        result = response.json()['response']
        print("üì§ –û—Ç–≤–µ—Ç –æ—Ç Ollama:", repr(result))
        return eval(result.strip()) if result.strip().startswith("[") else []

    except Exception as e:
        print("‚ùå –û—à–∏–±–∫–∞ Ollama:", e)
        return []

# üéß –û—Å–Ω–æ–≤–Ω–æ–π –º–∏–∫—à–∏—Ä–æ–≤–∞–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–æ–≤
def mix_effects(audio_path, fx_dir):
    voice = AudioSegment.from_mp3(audio_path)
    duration = len(voice)
    fx_track = AudioSegment.silent(duration=duration)

    result = model.transcribe(audio_path, language="ru", verbose=False)
    segments = result["segments"]

    events = []

    for seg in segments:
        print(f"üó£Ô∏è –°–µ–≥–º–µ–Ω—Ç: {seg['text']}")
        sfx_list = get_sfx_from_ollama(seg['text'])
        print(f"üéµ –ù–∞–π–¥–µ–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–æ–≤: {sfx_list}")
        for sound_file in sfx_list:
            events.append({
                "text": seg['text'],
                "sound": sound_file,
                "position": int(seg['start'] * 1000)
            })

    for event in events:
        fx_path = fx_dir / event["sound"]
        if fx_path.exists():
            fx = AudioSegment.from_wav(fx_path).fade_in(300).fade_out(500) - 10
            fx_track = fx_track.overlay(fx, position=event["position"])
        else:
            print(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω –∑–≤—É–∫: {fx_path}")

    final = voice.overlay(fx_track)
    final.export("result.wav", format="wav")
    print("‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω: result.wav")
    save_events_to_excel(events)

# üöÄ –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞
def main():
    audio_path = "audio/voice.mp3"
    fx_dir = Path("audio/fx")
    mix_effects(audio_path, fx_dir)

if __name__ == "__main__":
    main()